# [!NOTE]
# Read the `docs/configuration_guide.md` carefully, and update the configurations to match your specific settings and requirements.
# - Replace `api_key` with your own credentials
# - Replace `base_url` and `model` name if you want to use a custom model

# BASIC_MODEL:
#   base_url: https://llmproxy.gwm.cn/v1
#   model: default/qwen2-5-72b-instruct-awq
#   api_key: Ntie1k9gIqxcBCnIyKwpvo0mnMC/hYEwvjp5of0pri04ObPwvRXmIqWA6nthExCs9XXuYYcD0DUq
#   max_tokens: 8192

BASIC_MODEL:
  base_url: https://api.deepseek.com
  model: "deepseek-chat"
  api_key: sk-bcaa8925008649038924cb6a3f89f645
  max_tokens: 16384

BASIC_MODEL_LOW_TEMP:
  base_url: https://api.deepseek.com
  model: deepseek-chat
  api_key: sk-bcaa8925008649038924cb6a3f89f645
  max_tokens: 16384
  temperature: 1.2
  top_p: 1

# REASONING_MODEL:
#   base_url: https://llmproxy.gwm.cn/v1
#   model: default/qwen2-5-72b-instruct-awq
#   api_key: Ntie1k9gIqxcBCnIyKwpvo0mnMC/hYEwvjp5of0pri04ObPwvRXmIqWA6nthExCs9XXuYYcD0DUq
#   max_tokens: 8192

REASONING_MODEL:
  base_url: https://api.deepseek.com
  model: deepseek-reasoner
  api_key: sk-bcaa8925008649038924cb6a3f89f645
  max_tokens: 24576
  temperature: 1.2
  top_p: 0.8

MODEL_PROVIDER:
  #provider: "ollama"
  #provider: "deepseek"
  provider: "openai"

